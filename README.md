# Understanding Transformers: A From-Scratch PyTorch Implementation

<img width="337" alt="image" src="https://github.com/user-attachments/assets/6a6caba8-5e47-4a95-9962-f632624675e5" />


## About This Repository
Transformers have **revolutionized AI**, powering state-of-the-art models like **ChatGPT, BERT, and Vision Transformers**. But how do they actually work? Many practitioners fine-tune pre-trained models but **never get to build one from scratch**. This repository is a **step-by-step breakdown of the Transformer architecture**, implementing it from scratch in **PyTorch**.

This project is designed for anyone who wants to **go beyond just using Transformers** and truly understand their **inner workings**.

---

##  What‚Äôs Inside?
This repository includes a **fully implemented Transformer model**, covering both **theory and practical application**:

‚úÖ **Theoretical Overview**  
   - **Word Embeddings & Positional Encoding**  
   - **Self-Attention & Cross-Attention Mechanisms**  
   - **Feedforward Networks & Residual Connections**  
   - **Why Transformers Replace RNNs & LSTMs**  

‚úÖ **Implementation from Scratch (PyTorch)**  
   - **Custom Transformer Model**  
   - **Tokenization and Data Processing**  
   - **Training Pipeline**  

‚úÖ **Hands-On Application: English-to-French Translation** üá¨üáß ‚Üí üá´üá∑  
   - A practical **sequence-to-sequence task** demonstrating how the Transformer model can translate text from English to French.
---
## üñºÔ∏è Sample Visualizations
Here are some key insights from the notebook:

**Word Embeddings Representation**  
_(Shows how words are transformed into dense vectors in Transformer models.)_

<img width="313" alt="image" src="https://github.com/user-attachments/assets/715018f7-e926-4f5c-bcf1-32841beeebf4" />

**Cross-Attention Heatmap**  
_(Illustrates how English words align with their French counterparts in translation.)_

<img width="323" alt="image" src="https://github.com/user-attachments/assets/08541678-d257-4ad7-838f-f794852d5bee" />

---

##  Why This Project?
Many tutorials either **dive too deep into theory without code**, or just provide **pre-built Transformer implementations**. This project strikes a balance between **understanding the fundamental principles** and **practically implementing a Transformer from scratch**.

If you're looking to:
‚úî **Understand the math behind Transformers**  
‚úî **Build a Transformer model without relying on pre-built frameworks**  
‚úî **Visualize how attention works in NLP**  
This repository is for you! 

---

##  References
- ["Attention Is All You Need" (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)

---

### If this project helps you, consider giving it a ‚≠ê on GitHub!
 **Let‚Äôs demystify Transformers together!** 

